{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Y4o8AvPc51pdzcvNO9IB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiabaGodwin/0x01.c/blob/master/Advance_Machine_learning_%26_Deep_Learning_Masterclass_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypjpKjmlKElR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1d5192-3f7b-4fe7-f344-e36fba6dd7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3\n"
          ]
        }
      ],
      "source": [
        "#Working with a single input neuron\n",
        "\n",
        "inputs = [1.0,2.0,3.0]\n",
        "weights = [0.2, 0.8, -0.5]\n",
        "bias = 2.0\n",
        "output= inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+bias\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with multiple input neurons"
      ],
      "metadata": {
        "id": "Wu6SAz2emgVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1.0,2.0,3.0,2.5]\n",
        "weights = [[0.2, 0.8, -0.5,1],\n",
        "           [0.5,-0.91,0.26,-0.5],\n",
        "           [-0.26, -0.27,0.17,0.87]]\n",
        "bias1 = 2.0\n",
        "bias2 = 3.0\n",
        "bias3 = 0.5\n",
        "output = [inputs[0]*weights[0][0]+inputs[1]*weights[0][1]+inputs[2]*weights[0][2]+inputs[3]*weights[0][3]+bias1,\n",
        "          inputs[0]*weights[1][0]+inputs[1]*weights[1][1]+inputs[2]*weights[1][2]+inputs[3]*weights[1][3]+bias2,\n",
        "          inputs[0]*weights[2][0]+inputs[1]*weights[2][1]+inputs[2]*weights[2][2]+inputs[3]*weights[2][3]+bias3]\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixDNlSY_mqrH",
        "outputId": "24f7a2bb-e54f-42bc-e47e-c8993a13e74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/pima-indians-diabetes.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ClP-dbZhPaGE",
        "outputId": "6f4868c1-966d-4e84-c0cd-1534938835f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   6  148  72  35    0  33.6  0.627  50  1\n",
              "0  1   85  66  29    0  26.6  0.351  31  0\n",
              "1  8  183  64   0    0  23.3  0.672  32  1\n",
              "2  1   89  66  23   94  28.1  0.167  21  0\n",
              "3  0  137  40  35  168  43.1  2.288  33  1\n",
              "4  5  116  74   0    0  25.6  0.201  30  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-620a1f08-2805-4a1b-b4f2-fb0b624f5f8f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>6</th>\n",
              "      <th>148</th>\n",
              "      <th>72</th>\n",
              "      <th>35</th>\n",
              "      <th>0</th>\n",
              "      <th>33.6</th>\n",
              "      <th>0.627</th>\n",
              "      <th>50</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>116</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.6</td>\n",
              "      <td>0.201</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-620a1f08-2805-4a1b-b4f2-fb0b624f5f8f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-620a1f08-2805-4a1b-b4f2-fb0b624f5f8f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-620a1f08-2805-4a1b-b4f2-fb0b624f5f8f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0de16e53-2079-4d52-ba9d-b078644fbeb5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0de16e53-2079-4d52-ba9d-b078644fbeb5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0de16e53-2079-4d52-ba9d-b078644fbeb5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 767,\n  \"fields\": [\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 17,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1,\n          8,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"148\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 136,\n        \"samples\": [\n          96,\n          176,\n          113\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"72\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 0,\n        \"max\": 122,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          86,\n          46,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"35\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          7,\n          12,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115,\n        \"min\": 0,\n        \"max\": 846,\n        \"num_unique_values\": 186,\n        \"samples\": [\n          52,\n          41,\n          183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"33.6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.8890909013660195,\n        \"min\": 0.0,\n        \"max\": 67.1,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          27.6,\n          35.3,\n          38.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0.627\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33149735557642684,\n        \"min\": 0.078,\n        \"max\": 2.42,\n        \"num_unique_values\": 516,\n        \"samples\": [\n          0.545,\n          0.426,\n          0.593\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 21,\n        \"max\": 81,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          60,\n          47,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building neural network model in Keras"
      ],
      "metadata": {
        "id": "7ONi6JdZW8Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_fFLaAjW7rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# load the dataset\n",
        "dataset = loadtxt('/content/pima-indians-diabetes.csv', delimiter=',')\n",
        "# split into input (X) and output (y) variables\n",
        "X = dataset[:,0:8]\n",
        "y = dataset[:,8]"
      ],
      "metadata": {
        "id": "GDD8oC33QJ0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTowqzMQSiYS",
        "outputId": "773c9173-84ed-4424-ee4e-75b49ccbafda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PWDRou1eUwFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model on the dataset\n",
        "model.fit(X,y, epochs=150, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpb3Vy3-VTJB",
        "outputId": "6d7047cf-7397-41fa-b98d-f5b56a6942c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6265 - loss: 6.4857\n",
            "Epoch 2/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5581 - loss: 3.3979\n",
            "Epoch 3/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5999 - loss: 1.4579\n",
            "Epoch 4/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5945 - loss: 0.9765\n",
            "Epoch 5/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6264 - loss: 0.8571\n",
            "Epoch 6/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6367 - loss: 0.7883\n",
            "Epoch 7/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6772 - loss: 0.7130\n",
            "Epoch 8/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7035 - loss: 0.7021\n",
            "Epoch 9/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6910 - loss: 0.6880\n",
            "Epoch 10/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6771 - loss: 0.6237\n",
            "Epoch 11/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6886 - loss: 0.6872\n",
            "Epoch 12/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6773 - loss: 0.6330\n",
            "Epoch 13/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6750 - loss: 0.6447\n",
            "Epoch 14/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6979 - loss: 0.5929\n",
            "Epoch 15/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7173 - loss: 0.5812\n",
            "Epoch 16/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6887 - loss: 0.6371\n",
            "Epoch 17/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6764 - loss: 0.6686\n",
            "Epoch 18/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7099 - loss: 0.6400\n",
            "Epoch 19/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7144 - loss: 0.5959\n",
            "Epoch 20/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6921 - loss: 0.6212\n",
            "Epoch 21/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7169 - loss: 0.5611\n",
            "Epoch 22/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6990 - loss: 0.5983\n",
            "Epoch 23/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6858 - loss: 0.6499\n",
            "Epoch 24/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7161 - loss: 0.5862\n",
            "Epoch 25/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6920 - loss: 0.6185\n",
            "Epoch 26/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6975 - loss: 0.6021\n",
            "Epoch 27/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7005 - loss: 0.6066\n",
            "Epoch 28/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6970 - loss: 0.6080\n",
            "Epoch 29/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6887 - loss: 0.5679\n",
            "Epoch 30/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7202 - loss: 0.5691\n",
            "Epoch 31/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7289 - loss: 0.5551\n",
            "Epoch 32/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7128 - loss: 0.5645\n",
            "Epoch 33/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7150 - loss: 0.6001\n",
            "Epoch 34/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6986 - loss: 0.5975\n",
            "Epoch 35/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7159 - loss: 0.5696\n",
            "Epoch 36/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6818 - loss: 0.5926\n",
            "Epoch 37/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6838 - loss: 0.6341\n",
            "Epoch 38/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7390 - loss: 0.5472\n",
            "Epoch 39/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7192 - loss: 0.6421\n",
            "Epoch 40/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7209 - loss: 0.5784\n",
            "Epoch 41/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6937 - loss: 0.5965\n",
            "Epoch 42/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7178 - loss: 0.6278\n",
            "Epoch 43/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6948 - loss: 0.6367\n",
            "Epoch 44/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7159 - loss: 0.5551\n",
            "Epoch 45/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7126 - loss: 0.5616\n",
            "Epoch 46/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7382 - loss: 0.5686\n",
            "Epoch 47/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7262 - loss: 0.5509\n",
            "Epoch 48/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6871 - loss: 0.5532\n",
            "Epoch 49/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7028 - loss: 0.5953\n",
            "Epoch 50/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7292 - loss: 0.5308\n",
            "Epoch 51/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7239 - loss: 0.5716\n",
            "Epoch 52/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7070 - loss: 0.5920\n",
            "Epoch 53/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7069 - loss: 0.5999\n",
            "Epoch 54/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7335 - loss: 0.5457\n",
            "Epoch 55/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7511 - loss: 0.5307\n",
            "Epoch 56/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7153 - loss: 0.5593\n",
            "Epoch 57/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7109 - loss: 0.6030\n",
            "Epoch 58/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7265 - loss: 0.5658\n",
            "Epoch 59/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7205 - loss: 0.6414\n",
            "Epoch 60/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7147 - loss: 0.5635\n",
            "Epoch 61/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.5051\n",
            "Epoch 62/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7217 - loss: 0.5534\n",
            "Epoch 63/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7131 - loss: 0.5546\n",
            "Epoch 64/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7172 - loss: 0.5889\n",
            "Epoch 65/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7199 - loss: 0.5593\n",
            "Epoch 66/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7137 - loss: 0.5438\n",
            "Epoch 67/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7319 - loss: 0.5593\n",
            "Epoch 68/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7231 - loss: 0.5570\n",
            "Epoch 69/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7340 - loss: 0.5490\n",
            "Epoch 70/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7124 - loss: 0.5981\n",
            "Epoch 71/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7575 - loss: 0.5263\n",
            "Epoch 72/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7464 - loss: 0.4890\n",
            "Epoch 73/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7236 - loss: 0.5672\n",
            "Epoch 74/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7382 - loss: 0.5358\n",
            "Epoch 75/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7236 - loss: 0.5308\n",
            "Epoch 76/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7378 - loss: 0.5524\n",
            "Epoch 77/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7317 - loss: 0.5409\n",
            "Epoch 78/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7638 - loss: 0.4905\n",
            "Epoch 79/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7205 - loss: 0.5269\n",
            "Epoch 80/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7693 - loss: 0.4957\n",
            "Epoch 81/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7255 - loss: 0.5942\n",
            "Epoch 82/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6947 - loss: 0.5824\n",
            "Epoch 83/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7570 - loss: 0.4823\n",
            "Epoch 84/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7357 - loss: 0.5828\n",
            "Epoch 85/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7748 - loss: 0.4990\n",
            "Epoch 86/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7068 - loss: 0.6184\n",
            "Epoch 87/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7300 - loss: 0.5425\n",
            "Epoch 88/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7283 - loss: 0.5686\n",
            "Epoch 89/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7712 - loss: 0.4764\n",
            "Epoch 90/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7193 - loss: 0.5473\n",
            "Epoch 91/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7619 - loss: 0.4965\n",
            "Epoch 92/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7485 - loss: 0.5196\n",
            "Epoch 93/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7409 - loss: 0.5368\n",
            "Epoch 94/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7196 - loss: 0.5811\n",
            "Epoch 95/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7809 - loss: 0.4687\n",
            "Epoch 96/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7281 - loss: 0.5515\n",
            "Epoch 97/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7551 - loss: 0.5189\n",
            "Epoch 98/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7421 - loss: 0.5710\n",
            "Epoch 99/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7428 - loss: 0.5436\n",
            "Epoch 100/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7051 - loss: 0.6100\n",
            "Epoch 101/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7442 - loss: 0.5013\n",
            "Epoch 102/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7501 - loss: 0.4961\n",
            "Epoch 103/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7364 - loss: 0.5209\n",
            "Epoch 104/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7252 - loss: 0.5167\n",
            "Epoch 105/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7628 - loss: 0.4809\n",
            "Epoch 106/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7091 - loss: 0.5801\n",
            "Epoch 107/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7421 - loss: 0.5278\n",
            "Epoch 108/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7189 - loss: 0.5315\n",
            "Epoch 109/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.5039\n",
            "Epoch 110/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7376 - loss: 0.5172\n",
            "Epoch 111/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7536 - loss: 0.5170\n",
            "Epoch 112/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4808\n",
            "Epoch 113/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7542 - loss: 0.5092\n",
            "Epoch 114/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7395 - loss: 0.5524\n",
            "Epoch 115/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7329 - loss: 0.5123\n",
            "Epoch 116/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7482 - loss: 0.5569\n",
            "Epoch 117/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7216 - loss: 0.5339\n",
            "Epoch 118/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7382 - loss: 0.5010\n",
            "Epoch 119/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7438 - loss: 0.5472\n",
            "Epoch 120/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.5252\n",
            "Epoch 121/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7106 - loss: 0.5542\n",
            "Epoch 122/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7347 - loss: 0.5166\n",
            "Epoch 123/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7726 - loss: 0.4830\n",
            "Epoch 124/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7116 - loss: 0.5444\n",
            "Epoch 125/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7439 - loss: 0.4977\n",
            "Epoch 126/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7152 - loss: 0.5525\n",
            "Epoch 127/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7191 - loss: 0.5563\n",
            "Epoch 128/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7251 - loss: 0.5325\n",
            "Epoch 129/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7426 - loss: 0.4913\n",
            "Epoch 130/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7535 - loss: 0.5062\n",
            "Epoch 131/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7690 - loss: 0.4809\n",
            "Epoch 132/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7378 - loss: 0.5186\n",
            "Epoch 133/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7622 - loss: 0.4942\n",
            "Epoch 134/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7589 - loss: 0.5034\n",
            "Epoch 135/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7296 - loss: 0.5656\n",
            "Epoch 136/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7475 - loss: 0.4976\n",
            "Epoch 137/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7871 - loss: 0.4829\n",
            "Epoch 138/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7318 - loss: 0.5202\n",
            "Epoch 139/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7233 - loss: 0.5766\n",
            "Epoch 140/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7521 - loss: 0.5131\n",
            "Epoch 141/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7386 - loss: 0.5240\n",
            "Epoch 142/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7309 - loss: 0.5192\n",
            "Epoch 143/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7379 - loss: 0.5389\n",
            "Epoch 144/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.5009\n",
            "Epoch 145/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7430 - loss: 0.5096\n",
            "Epoch 146/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7591 - loss: 0.4973\n",
            "Epoch 147/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7797 - loss: 0.5102\n",
            "Epoch 148/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7608 - loss: 0.4743\n",
            "Epoch 149/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7424 - loss: 0.5168\n",
            "Epoch 150/150\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7180 - loss: 0.5624\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ac1d47ccf40>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evalute the model\n",
        "\n",
        "_, accuracy = model.evaluate(X,y)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNl_YHsOWC6o",
        "outputId": "cbfc292f-d590-4e0c-e22e-da0de563094d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7437 - loss: 0.5266 \n",
            "Accuracy: 75.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadwritten Digit Recognition Project(Hello world project in deeplearning)"
      ],
      "metadata": {
        "id": "C4dbyB8Rckzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist #mnist dataset of 28x28 images of handwriiten digit and their labels\n",
        "(x_train, y_train),(x_test,y_test) = mnist.load_data()\n",
        "\n",
        "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
        "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
        "\n",
        "#A sequestial model is what youre going to use most of the time. It just means things are going to in direct order.\n",
        "#A food  forward model. No going backwards for now\n",
        "\n",
        "model = tf.keras.models.Sequential()  #A basic feed forward model\n",
        "model.add(tf.keras.layers.Flatten())  #takes our 28x28 and make it 1x784\n",
        "\n",
        "\n",
        "#This layer has 128 units. The activation function is relu, short for rectified linear.\n",
        "#currently, relu is the activation function you should just default to\n",
        "# There are many more to test for sure, but if you don't know what to use, use relu to start\n",
        "model.add(tf.keras.layers.Dense(128, activation= tf.nn.relu))# A simple fully-connected layer, 128 units, relu activation function\n",
        "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
        "\n",
        "#this is our final layer. It has 10 nodes\n",
        "#1 node  per possible numbeer prediction\n",
        "#in this case, our activation function  is a softmax, since we'er  really actualy looking for something more like a probability distribution  of which the possible options this thing we're passing features through\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))  # our output layer. 10 n=units for 10 classes. Softmax for probability distribution\n",
        "\n",
        "model.compile(optimizer='adam'  #good default optimizer to start with\n",
        ", loss='sparse_categorical_crossentropy', #how will we calculate our \"error\". Neural network aims at\n",
        "              metrics=['accuracy']) #what to track\n",
        "\n",
        "model.fit(x_train, y_train, epochs=3) #train the model\n",
        "val_loss, val_acc = model.evaluate(x_test,y_test)  #evaluate the out of sample data whit model\n",
        "print(val_loss)\n",
        "print(val_acc)\n",
        "\n",
        "model.save('epic_num_reader.keras')\n",
        "\n",
        "new_model = tf.keras.models.load_model('epic_num_reader.keras')\n",
        "\n",
        "prediction = new_model.predict(x_test)\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "gPftgd--W4tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corrected code by Gemini"
      ],
      "metadata": {
        "id": "tBsulPAiO8gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
        "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))  # Use 'relu' instead of tf.nn.relu\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))  # Use 'relu' instead of tf.nn.relu\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))  # Use 'softmax' instead of tf.nn.softmax\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=3)\n",
        "\n",
        "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
        "print(val_loss)\n",
        "print(val_acc)\n",
        "\n",
        "model.save('epic_num_reader.keras')  # Save with .keras extension\n",
        "\n",
        "new_model = tf.keras.models.load_model('epic_num_reader.keras')  # Load with .keras extension\n",
        "\n",
        "predictions = new_model.predict(x_test)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apvs4UpfOsJI",
        "outputId": "cc63004d-3e4a-4b81-abb8-c7a22419e4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8604 - loss: 0.4874\n",
            "Epoch 2/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9631 - loss: 0.1228\n",
            "Epoch 3/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9770 - loss: 0.0754\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9619 - loss: 0.1199\n",
            "0.10657304525375366\n",
            "0.9678999781608582\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[[1.7602025e-08 4.0404316e-07 2.1164444e-05 ... 9.9992698e-01\n",
            "  7.6792691e-07 7.0455496e-07]\n",
            " [2.2452740e-09 9.3395717e-04 9.9905628e-01 ... 4.0001304e-09\n",
            "  2.7221520e-06 2.0933243e-13]\n",
            " [2.6425639e-07 9.9945080e-01 2.3044435e-05 ... 7.5784374e-05\n",
            "  3.6019689e-04 4.3593320e-07]\n",
            " ...\n",
            " [2.6106576e-08 3.5845405e-06 5.2001880e-08 ... 1.9937574e-05\n",
            "  2.5741232e-04 9.8680416e-03]\n",
            " [8.8997473e-07 6.7645074e-06 5.8285474e-09 ... 6.6479252e-08\n",
            "  8.3018444e-04 3.8827409e-08]\n",
            " [4.3595108e-08 4.5010289e-08 1.0180435e-06 ... 2.4762947e-10\n",
            "  2.5755212e-06 2.1214981e-11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(np.argmax(predictions[565]))\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_test[565],cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "VUZzFGhYPcuz",
        "outputId": "b908a52c-c716-4e98-a90d-41c135a21076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_test[0],cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pdxs5U-FQOxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive bayes"
      ],
      "metadata": {
        "id": "SCrMkKi2HvhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import urllib\n",
        "import urllib.request as ur\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import metrics\n",
        "url='https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
        "raw_data=ur.urlopen(url)\n",
        "\n",
        "dataset = np.loadtxt(raw_data, delimiter=',')\n",
        "print(dataset[0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J92HnYk2Hu-0",
        "outputId": "23d60570-7c19-4e70-88ff-6245a66c5c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0.      0.64    0.64    0.      0.32    0.      0.      0.      0.\n",
            "   0.      0.      0.64    0.      0.      0.      0.32    0.      1.29\n",
            "   1.93    0.      0.96    0.      0.      0.      0.      0.      0.\n",
            "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
            "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
            "   0.      0.      0.      0.      0.      0.      0.778   0.      0.\n",
            "   3.756  61.    278.      1.   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=dataset[:,0:48]\n",
        "y=dataset[:,-1]\n"
      ],
      "metadata": {
        "id": "uhnnxwCqKqqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,x_test,y_train,y_test= train_test_split(X,y,test_size=.33, random_state=17)"
      ],
      "metadata": {
        "id": "IjC-8uBoLLLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BernNB = BernoulliNB(binarize=True)\n",
        "BernNB.fit(X_train,y_train)\n",
        "print(BernNB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_2QbRTXL8Ej",
        "outputId": "e9223173-2582-43cb-f1b1-0bfe6632a987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BernoulliNB(binarize=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_expect = y_test\n",
        "y_pred = BernNB.predict(x_test)\n",
        "print(metrics.accuracy_score(y_expect,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xPQWmCdMVze",
        "outputId": "a2ee936a-790f-4e84-abbb-72c86f0c7a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8558262014483212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#this is an example of how the y_expect is compared to y_pred. Checking if the predicted value is equal to the expected value\n",
        "print(metrics.accuracy_score([1,1,0],[1,0,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lp7V9k2NuE1",
        "outputId": "fea6384b-d6e5-41ec-e9b3-e2b6a9bac5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Multinomial"
      ],
      "metadata": {
        "id": "I6ZXr5O9OzZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MultiNB = MultinomialNB()\n",
        "MultiNB.fit(X_train, y_train)\n",
        "\n",
        "y_pred = MultiNB.predict(x_test)\n",
        "print(metrics.accuracy_score(y_expect,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jfrmEdsO2N7",
        "outputId": "8b5e6c2b-07c6-4e9e-a2c1-add73792fe3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8736010533245556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the Gausian naive bayes\n",
        "GausNB = GaussianNB()\n",
        "GausNB.fit(X_train, y_train)\n",
        "\n",
        "y_pred = GausNB.predict(x_test)\n",
        "print(metrics.accuracy_score(y_expect,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuz704mZPvFx",
        "outputId": "48f6ed8f-f566-4630-d997-4691409adaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8130348913759052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language processing Session"
      ],
      "metadata": {
        "id": "OJxHWZ5LRiQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh2zWauOB7VP",
        "outputId": "35bd7cf7-19c9-4141-83b5-871fb84a879c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sample_text = 'Flagging is usually don to draw attention to the log synced. Is used for indicating the local log has been syned to the db'\n",
        "tokenized_sentence = word_tokenize(sample_text)\n",
        "for sentence in tokenized_sentence:\n",
        "  print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrnceQCsW7UM",
        "outputId": "51d2e775-4550-4b03-ff4b-b80e36235996"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flagging\n",
            "is\n",
            "usually\n",
            "don\n",
            "to\n",
            "draw\n",
            "attention\n",
            "to\n",
            "the\n",
            "log\n",
            "synced\n",
            ".\n",
            "Is\n",
            "used\n",
            "for\n",
            "indicating\n",
            "the\n",
            "local\n",
            "log\n",
            "has\n",
            "been\n",
            "syned\n",
            "to\n",
            "the\n",
            "db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = ['illegal', 'walking','eating']\n",
        "for wod in words:\n",
        "  print(ps.stem(wod))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLqErag1DXis",
        "outputId": "9582915a-9276-499b-cd16-5ac66c8b6dab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "illeg\n",
            "walk\n",
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB41C8HFIxah",
        "outputId": "421c66d4-6f50-4b78-ba76-d17c225286d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "sample_text= 'I was walking by the road when i saw the cutest puppy on the other side'\n",
        "\n",
        "tokenized_word= word_tokenize(sample_text)\n",
        "tag = pos_tag(tokenized_word)\n",
        "print(tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d_vCNj47HYQ",
        "outputId": "ffb92b78-7edf-446e-a929-046de499c382"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('was', 'VBD'), ('walking', 'VBG'), ('by', 'IN'), ('the', 'DT'), ('road', 'NN'), ('when', 'WRB'), ('i', 'NN'), ('saw', 'VBD'), ('the', 'DT'), ('cutest', 'JJS'), ('puppy', 'NN'), ('on', 'IN'), ('the', 'DT'), ('other', 'JJ'), ('side', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('2001aspaceodyssey.txt','r') as corpora:\n",
        "  sample_text = corpora.read()\n",
        "\n",
        "# print(sample_text)\n"
      ],
      "metadata": {
        "id": "KsQ3MPZ4Ly2X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_doc = sent_tokenize(sample_text)\n",
        "\n",
        "for sentence in tokenized_doc:\n",
        "  tokenized_words= word_tokenize(sample_text)\n",
        "  tag = pos_tag(tokenized_words)\n",
        "  # print(tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "UKWWJqVfM9qc",
        "outputId": "6bccf62b-a08e-43e0-84aa-9c3c6c682325"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c854de6aca17>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_doc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtokenized_words\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m# print(tag)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m    165\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajmep_Irkzh7",
        "outputId": "9d76c356-c2db-4d47-8a07-82d077c99818"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words\n",
        "from nltk.corpus import stopwords\n",
        "txt = \"Stop words are a cool way of getting rid of words we do not use\"\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n",
        "\n",
        "tokenized_words = word_tokenize(txt)\n",
        "filtered_sentence =[]\n",
        "\n",
        "for word in tokenized_words:\n",
        "  if word not in stop_words:\n",
        "    filtered_sentence.append(word)\n",
        "\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdFFjBBbe6yi",
        "outputId": "ca5d615c-2650-427e-c5b4-bb36767cd785"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'it', \"wouldn't\", 'no', 'she', 'over', 'having', 'him', 'and', \"doesn't\", 'an', 'with', 'from', 'each', 'more', 'mustn', \"won't\", 'if', 'up', 'been', 'ours', 'very', 'those', 'off', 'were', 'her', 'itself', 'what', 'once', 'that', 'did', \"hasn't\", 'hers', 'them', 'needn', 'had', 'the', 'now', 'doing', 'haven', \"you'd\", \"needn't\", 'further', \"it's\", 'i', 'should', 'aren', \"you'll\", 'this', 'any', 'just', 'both', 'hasn', 'we', 'wasn', \"didn't\", 'here', 'again', 't', \"don't\", 'but', 'how', 'not', 'you', 'm', \"should've\", 's', 'a', 'than', 'y', 'ain', \"mustn't\", \"isn't\", 'or', 'during', 'my', 'his', \"couldn't\", 'couldn', 'there', 'which', 'while', 'then', 'do', 'into', 'be', 'where', 'for', 'such', 'between', 'he', \"weren't\", 'yours', 'down', 'these', 'most', 'so', 'can', 'theirs', 'didn', 'ma', 'they', 'until', 'nor', 'too', 'being', 'against', 'was', \"you're\", 'out', 'about', 'their', 'our', \"that'll\", \"shouldn't\", \"wasn't\", 'does', 'other', 'will', 've', 'whom', 'me', 'himself', \"you've\", 'some', 'its', 'as', 'below', 'll', 'to', 'ourselves', 'why', 'herself', 'same', 're', 'yourself', 'are', 'in', 'who', 'don', \"hadn't\", 'at', 'shouldn', 'myself', 'o', 'yourselves', 'isn', 'all', 'd', 'won', 'by', 'above', 'through', \"she's\", 'have', 'of', 'because', 'after', 'has', \"aren't\", 'your', \"shan't\", 'when', 'mightn', 'under', 'few', 'am', 'on', 'is', 'doesn', 'weren', \"mightn't\", 'wouldn', 'own', \"haven't\", 'only', 'before', 'hadn', 'themselves', 'shan'}\n",
            "['Stop', 'words', 'cool', 'way', 'getting', 'rid', 'words', 'use']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlBgDcvKlFRp",
        "outputId": "f200d60a-6c5c-42e6-bdaf-d1a1fb3eb347"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named entity Recognition\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "\n",
        "txt = \"Trump  has won the election in the USA after a long battle with Kamala Haris\"\n",
        "tknized_wrd = word_tokenize(txt)\n",
        "tag = pos_tag(tknized_wrd)\n",
        "\n",
        "named_ent = nltk.ne_chunk(tag)\n",
        "print(named_ent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2faQuaS6jhKu",
        "outputId": "297a496f-6c0c-4471-9a8b-90fbb02595ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Trump/NNP)\n",
            "  has/VBZ\n",
            "  won/VBN\n",
            "  the/DT\n",
            "  election/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION USA/NNP)\n",
            "  after/IN\n",
            "  a/DT\n",
            "  long/JJ\n",
            "  battle/NN\n",
            "  with/IN\n",
            "  (PERSON Kamala/NNP Haris/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie reviews"
      ],
      "metadata": {
        "id": "WE8F5pqa9s1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN4vOLyD-8of",
        "outputId": "70406780-fe65-4e4a-ac92-d99c0ec57cb6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "#(['this','movies','is','great'], 'pos')\n",
        "\n",
        "documents = []\n",
        "for category in movie_reviews.categories():\n",
        "  for fileid in movie_reviews.fileids(category):\n",
        "    #  print(fileid)\n",
        "    # Use movie_reviews.words(fileid) to get words from the file\n",
        "    review_word_list = list(movie_reviews.words(fileid))\n",
        "    document = (review_word_list,category)\n",
        "    documents.append(document)\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "all_words = []\n",
        "for w in movie_reviews.words():\n",
        "  all_words.append(w.lower())\n",
        "\n",
        "# using nltk frequency dist to find the most common words\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "# print(all_words.most_common(10)) # Use most_common to get the top 10 words\n",
        "\n",
        "\n",
        "# Selecting Word Features\n",
        "word_features =[]\n",
        "for common_word in all_words.most_common(1000):\n",
        "  word_features.append(common_word[0])\n",
        "\n",
        "\n",
        "#Creating a feature extraction\n",
        "def find_features(feature_doc): #Converts the document into a set for efficient lookup.\n",
        "  words = set(feature_doc)\n",
        "  features= {}  #Creates an empty dictionary to store the features.\n",
        "  for word in word_features:\n",
        "    is_feature_in_words = word in words\n",
        "    features[word] = is_feature_in_words\n",
        "  return features\n",
        "\n",
        "\n",
        "# print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n",
        "\n",
        "featuresets = []\n",
        "for(review, category) in documents:\n",
        "  feature = (find_features(review), category)\n",
        "  featuresets.append(feature)\n",
        "\n",
        "\n",
        "\n",
        "training_set = featuresets[:100]\n",
        "test_set = featuresets[900:]\n",
        "\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
        "print(\"Accuracy: \", nltk.classify.accuracy(classifier,test_set))\n",
        "\n",
        "classifier.show_most_informative_features(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7q2lqPUs90GZ",
        "outputId": "7623153d-408a-445a-d69c-40e33be0820a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.6672727272727272\n",
            "Most Informative Features\n",
            "                american = True              pos : neg    =      7.3 : 1.0\n",
            "                    fast = True              neg : pos    =      6.6 : 1.0\n",
            "                  stupid = True              neg : pos    =      6.6 : 1.0\n",
            "                       t = False             pos : neg    =      5.8 : 1.0\n",
            "                   jokes = True              neg : pos    =      5.7 : 1.0\n",
            "                    mess = True              neg : pos    =      5.7 : 1.0\n",
            "                    ones = True              neg : pos    =      5.7 : 1.0\n",
            "                    teen = True              neg : pos    =      5.7 : 1.0\n",
            "                    lack = True              neg : pos    =      5.5 : 1.0\n",
            "                   worst = True              neg : pos    =      5.5 : 1.0\n",
            "                    fans = True              neg : pos    =      4.8 : 1.0\n",
            "                 neither = True              neg : pos    =      4.8 : 1.0\n",
            "               situation = True              neg : pos    =      4.8 : 1.0\n",
            "                   oscar = True              pos : neg    =      4.8 : 1.0\n",
            "                     die = True              neg : pos    =      4.5 : 1.0\n"
          ]
        }
      ]
    }
  ]
}